{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an implementation of the Byte-Pair Encoding Compression Algorithm (which is out of the scope of the book). This BPE tokenization method is used to train the GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/75/d5/294a09a62bdd88da9a1007a341d4f8fbfc43be520c101e6afb526000e9f4/transformers-4.46.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.1/44.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.23.2 from https://files.pythonhosted.org/packages/60/bf/cea0b9720c32fa01b0c4ec4b16b9f4ae34ca106b202ebbae9f03ab98cd8f/huggingface_hub-0.26.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/ce/00/a4bdf45a5f2e1db08aaf95bb97f8ca30ec9568573eda03ec0db9ce5ed5d2/safetensors-0.4.5-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.5-cp39-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.21,>=0.20 from https://files.pythonhosted.org/packages/31/95/ba6461cbb58e38d725564a74ce1ec56c63cb4abf13131ec400522d4f7ce8/tokenizers-0.20.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.20.1-cp39-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vani\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/10.0 MB 20.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.4/10.0 MB 14.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.7/10.0 MB 11.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.3/10.0 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.7/10.0 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.1/10.0 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.4/10.0 MB 9.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.7/10.0 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.1/10.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.5/10.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.9/10.0 MB 9.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.3/10.0 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.8/10.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.0/10.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.4/10.0 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.9/10.0 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.4/10.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.9/10.0 MB 9.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.3/10.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.8/10.0 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/10.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 9.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "   ---------------------------------------- 0.0/447.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 447.5/447.5 kB 9.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp39-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 286.1/286.1 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.20.1-cp39-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.6/2.4 MB 13.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.2/2.4 MB 12.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.4 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 13.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 12.6 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.26.2 safetensors-0.4.5 tokenizers-0.20.1 transformers-4.46.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\n",
    "    \"The cat sat quietly by the window, watching the birds outside.\",\n",
    "    \"She enjoys reading books on sunny afternoons in the park.\",\n",
    "    \"He picked fresh apples from the orchard and shared them with friends.\",\n",
    "    \"A warm breeze drifted through the open door as the sun set.\",\n",
    "    \"The children played games and laughed together on the grassy field.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vani\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Vani\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 2, 'Ġcat': 1, 'Ġsat': 1, 'Ġquietly': 1, 'Ġby': 1, 'Ġthe': 7, 'Ġwindow': 1, ',': 1, 'Ġwatching': 1, 'Ġbirds': 1, 'Ġoutside': 1, '.': 5, 'She': 1, 'Ġenjoys': 1, 'Ġreading': 1, 'Ġbooks': 1, 'Ġon': 2, 'Ġsunny': 1, 'Ġafternoons': 1, 'Ġin': 1, 'Ġpark': 1, 'He': 1, 'Ġpicked': 1, 'Ġfresh': 1, 'Ġapples': 1, 'Ġfrom': 1, 'Ġorchard': 1, 'Ġand': 2, 'Ġshared': 1, 'Ġthem': 1, 'Ġwith': 1, 'Ġfriends': 1, 'A': 1, 'Ġwarm': 1, 'Ġbreeze': 1, 'Ġdrifted': 1, 'Ġthrough': 1, 'Ġopen': 1, 'Ġdoor': 1, 'Ġas': 1, 'Ġsun': 1, 'Ġset': 1, 'Ġchildren': 1, 'Ġplayed': 1, 'Ġgames': 1, 'Ġlaughed': 1, 'Ġtogether': 1, 'Ġgrassy': 1, 'Ġfield': 1}\n"
     ]
    }
   ],
   "source": [
    "word_freq = {}\n",
    "for sentence in corpus:\n",
    "    words_with_offset=tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(sentence)\n",
    "    new_words=[word for word,offset in words_with_offset]\n",
    "    for word in new_words:\n",
    "        word_freq[word] = 1 + word_freq.get(word,0)\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'A', 'H', 'S', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "alphabet=[]\n",
    "\n",
    "for word in word_freq:\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': ['T', 'h', 'e'], 'Ġcat': ['Ġ', 'c', 'a', 't'], 'Ġsat': ['Ġ', 's', 'a', 't'], 'Ġquietly': ['Ġ', 'q', 'u', 'i', 'e', 't', 'l', 'y'], 'Ġby': ['Ġ', 'b', 'y'], 'Ġthe': ['Ġ', 't', 'h', 'e'], 'Ġwindow': ['Ġ', 'w', 'i', 'n', 'd', 'o', 'w'], ',': [','], 'Ġwatching': ['Ġ', 'w', 'a', 't', 'c', 'h', 'i', 'n', 'g'], 'Ġbirds': ['Ġ', 'b', 'i', 'r', 'd', 's'], 'Ġoutside': ['Ġ', 'o', 'u', 't', 's', 'i', 'd', 'e'], '.': ['.'], 'She': ['S', 'h', 'e'], 'Ġenjoys': ['Ġ', 'e', 'n', 'j', 'o', 'y', 's'], 'Ġreading': ['Ġ', 'r', 'e', 'a', 'd', 'i', 'n', 'g'], 'Ġbooks': ['Ġ', 'b', 'o', 'o', 'k', 's'], 'Ġon': ['Ġ', 'o', 'n'], 'Ġsunny': ['Ġ', 's', 'u', 'n', 'n', 'y'], 'Ġafternoons': ['Ġ', 'a', 'f', 't', 'e', 'r', 'n', 'o', 'o', 'n', 's'], 'Ġin': ['Ġ', 'i', 'n'], 'Ġpark': ['Ġ', 'p', 'a', 'r', 'k'], 'He': ['H', 'e'], 'Ġpicked': ['Ġ', 'p', 'i', 'c', 'k', 'e', 'd'], 'Ġfresh': ['Ġ', 'f', 'r', 'e', 's', 'h'], 'Ġapples': ['Ġ', 'a', 'p', 'p', 'l', 'e', 's'], 'Ġfrom': ['Ġ', 'f', 'r', 'o', 'm'], 'Ġorchard': ['Ġ', 'o', 'r', 'c', 'h', 'a', 'r', 'd'], 'Ġand': ['Ġ', 'a', 'n', 'd'], 'Ġshared': ['Ġ', 's', 'h', 'a', 'r', 'e', 'd'], 'Ġthem': ['Ġ', 't', 'h', 'e', 'm'], 'Ġwith': ['Ġ', 'w', 'i', 't', 'h'], 'Ġfriends': ['Ġ', 'f', 'r', 'i', 'e', 'n', 'd', 's'], 'A': ['A'], 'Ġwarm': ['Ġ', 'w', 'a', 'r', 'm'], 'Ġbreeze': ['Ġ', 'b', 'r', 'e', 'e', 'z', 'e'], 'Ġdrifted': ['Ġ', 'd', 'r', 'i', 'f', 't', 'e', 'd'], 'Ġthrough': ['Ġ', 't', 'h', 'r', 'o', 'u', 'g', 'h'], 'Ġopen': ['Ġ', 'o', 'p', 'e', 'n'], 'Ġdoor': ['Ġ', 'd', 'o', 'o', 'r'], 'Ġas': ['Ġ', 'a', 's'], 'Ġsun': ['Ġ', 's', 'u', 'n'], 'Ġset': ['Ġ', 's', 'e', 't'], 'Ġchildren': ['Ġ', 'c', 'h', 'i', 'l', 'd', 'r', 'e', 'n'], 'Ġplayed': ['Ġ', 'p', 'l', 'a', 'y', 'e', 'd'], 'Ġgames': ['Ġ', 'g', 'a', 'm', 'e', 's'], 'Ġlaughed': ['Ġ', 'l', 'a', 'u', 'g', 'h', 'e', 'd'], 'Ġtogether': ['Ġ', 't', 'o', 'g', 'e', 't', 'h', 'e', 'r'], 'Ġgrassy': ['Ġ', 'g', 'r', 'a', 's', 's', 'y'], 'Ġfield': ['Ġ', 'f', 'i', 'e', 'l', 'd']}\n"
     ]
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_freq.keys()}\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freq(splits):\n",
    "    pair_freqs={}\n",
    "    for word,freq in word_freq.items():\n",
    "        split=splits[word]\n",
    "        if len(split)==1:\n",
    "            continue\n",
    "        for i in range(len(split)-1):\n",
    "            pair=(split[i],split[i+1])\n",
    "            pair_freqs[pair] = freq + pair_freqs.get(pair,0)\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('T', 'h'): 2, ('h', 'e'): 13, ('Ġ', 'c'): 2, ('c', 'a'): 1, ('a', 't'): 3, ('Ġ', 's'): 5, ('s', 'a'): 1, ('Ġ', 'q'): 1, ('q', 'u'): 1, ('u', 'i'): 1, ('i', 'e'): 3, ('e', 't'): 3, ('t', 'l'): 1, ('l', 'y'): 1, ('Ġ', 'b'): 4, ('b', 'y'): 1, ('Ġ', 't'): 10, ('t', 'h'): 11, ('Ġ', 'w'): 4, ('w', 'i'): 2, ('i', 'n'): 4, ('n', 'd'): 4, ('d', 'o'): 2, ('o', 'w'): 1, ('w', 'a'): 2, ('t', 'c'): 1, ('c', 'h'): 3, ('h', 'i'): 2, ('n', 'g'): 2, ('b', 'i'): 1, ('i', 'r'): 1, ('r', 'd'): 2, ('d', 's'): 2, ('Ġ', 'o'): 5, ('o', 'u'): 2, ('u', 't'): 1, ('t', 's'): 1, ('s', 'i'): 1, ('i', 'd'): 1, ('d', 'e'): 1, ('S', 'h'): 1, ('Ġ', 'e'): 1, ('e', 'n'): 4, ('n', 'j'): 1, ('j', 'o'): 1, ('o', 'y'): 1, ('y', 's'): 1, ('Ġ', 'r'): 1, ('r', 'e'): 5, ('e', 'a'): 1, ('a', 'd'): 1, ('d', 'i'): 1, ('b', 'o'): 1, ('o', 'o'): 3, ('o', 'k'): 1, ('k', 's'): 1, ('o', 'n'): 3, ('s', 'u'): 2, ('u', 'n'): 2, ('n', 'n'): 1, ('n', 'y'): 1, ('Ġ', 'a'): 5, ('a', 'f'): 1, ('f', 't'): 2, ('t', 'e'): 2, ('e', 'r'): 2, ('r', 'n'): 1, ('n', 'o'): 1, ('n', 's'): 1, ('Ġ', 'i'): 1, ('Ġ', 'p'): 3, ('p', 'a'): 1, ('a', 'r'): 4, ('r', 'k'): 1, ('H', 'e'): 1, ('p', 'i'): 1, ('i', 'c'): 1, ('c', 'k'): 1, ('k', 'e'): 1, ('e', 'd'): 5, ('Ġ', 'f'): 4, ('f', 'r'): 3, ('e', 's'): 3, ('s', 'h'): 2, ('a', 'p'): 1, ('p', 'p'): 1, ('p', 'l'): 2, ('l', 'e'): 1, ('r', 'o'): 2, ('o', 'm'): 1, ('o', 'r'): 2, ('r', 'c'): 1, ('h', 'a'): 2, ('a', 'n'): 2, ('e', 'm'): 1, ('i', 't'): 1, ('r', 'i'): 2, ('r', 'm'): 1, ('b', 'r'): 1, ('e', 'e'): 1, ('e', 'z'): 1, ('z', 'e'): 1, ('Ġ', 'd'): 2, ('d', 'r'): 2, ('i', 'f'): 1, ('h', 'r'): 1, ('u', 'g'): 2, ('g', 'h'): 2, ('o', 'p'): 1, ('p', 'e'): 1, ('a', 's'): 2, ('s', 'e'): 1, ('i', 'l'): 1, ('l', 'd'): 2, ('l', 'a'): 2, ('a', 'y'): 1, ('y', 'e'): 1, ('Ġ', 'g'): 2, ('g', 'a'): 1, ('a', 'm'): 1, ('m', 'e'): 1, ('Ġ', 'l'): 1, ('a', 'u'): 1, ('t', 'o'): 1, ('o', 'g'): 1, ('g', 'e'): 1, ('g', 'r'): 1, ('r', 'a'): 1, ('s', 's'): 1, ('s', 'y'): 1, ('f', 'i'): 1, ('e', 'l'): 1}\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freq(splits)\n",
    "print(pair_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('h', 'e') 13\n"
     ]
    }
   ],
   "source": [
    "best_pair =\"\"\n",
    "max_freq = -1\n",
    "\n",
    "for pair,freq in pair_freqs.items():\n",
    "    if max_freq<freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {(\"h\",\"e\"):'he'}\n",
    "vocab.append('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pairs(a,b,splits):\n",
    "    for word in word_freq:\n",
    "        split = splits[word]\n",
    "        if len(split)==1:\n",
    "            continue\n",
    "\n",
    "        i=0\n",
    "        while i<len(split)-1:\n",
    "            if split[i]==a and split[i+1]==b:\n",
    "                split=split[:i] + [a+b] + split[i+2:]\n",
    "            else:\n",
    "                i+=1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': ['T', 'he'], 'Ġcat': ['Ġ', 'c', 'a', 't'], 'Ġsat': ['Ġ', 's', 'a', 't'], 'Ġquietly': ['Ġ', 'q', 'u', 'i', 'e', 't', 'l', 'y'], 'Ġby': ['Ġ', 'b', 'y'], 'Ġthe': ['Ġ', 't', 'he'], 'Ġwindow': ['Ġ', 'w', 'i', 'n', 'd', 'o', 'w'], ',': [','], 'Ġwatching': ['Ġ', 'w', 'a', 't', 'c', 'h', 'i', 'n', 'g'], 'Ġbirds': ['Ġ', 'b', 'i', 'r', 'd', 's'], 'Ġoutside': ['Ġ', 'o', 'u', 't', 's', 'i', 'd', 'e'], '.': ['.'], 'She': ['S', 'he'], 'Ġenjoys': ['Ġ', 'e', 'n', 'j', 'o', 'y', 's'], 'Ġreading': ['Ġ', 'r', 'e', 'a', 'd', 'i', 'n', 'g'], 'Ġbooks': ['Ġ', 'b', 'o', 'o', 'k', 's'], 'Ġon': ['Ġ', 'o', 'n'], 'Ġsunny': ['Ġ', 's', 'u', 'n', 'n', 'y'], 'Ġafternoons': ['Ġ', 'a', 'f', 't', 'e', 'r', 'n', 'o', 'o', 'n', 's'], 'Ġin': ['Ġ', 'i', 'n'], 'Ġpark': ['Ġ', 'p', 'a', 'r', 'k'], 'He': ['H', 'e'], 'Ġpicked': ['Ġ', 'p', 'i', 'c', 'k', 'e', 'd'], 'Ġfresh': ['Ġ', 'f', 'r', 'e', 's', 'h'], 'Ġapples': ['Ġ', 'a', 'p', 'p', 'l', 'e', 's'], 'Ġfrom': ['Ġ', 'f', 'r', 'o', 'm'], 'Ġorchard': ['Ġ', 'o', 'r', 'c', 'h', 'a', 'r', 'd'], 'Ġand': ['Ġ', 'a', 'n', 'd'], 'Ġshared': ['Ġ', 's', 'h', 'a', 'r', 'e', 'd'], 'Ġthem': ['Ġ', 't', 'he', 'm'], 'Ġwith': ['Ġ', 'w', 'i', 't', 'h'], 'Ġfriends': ['Ġ', 'f', 'r', 'i', 'e', 'n', 'd', 's'], 'A': ['A'], 'Ġwarm': ['Ġ', 'w', 'a', 'r', 'm'], 'Ġbreeze': ['Ġ', 'b', 'r', 'e', 'e', 'z', 'e'], 'Ġdrifted': ['Ġ', 'd', 'r', 'i', 'f', 't', 'e', 'd'], 'Ġthrough': ['Ġ', 't', 'h', 'r', 'o', 'u', 'g', 'h'], 'Ġopen': ['Ġ', 'o', 'p', 'e', 'n'], 'Ġdoor': ['Ġ', 'd', 'o', 'o', 'r'], 'Ġas': ['Ġ', 'a', 's'], 'Ġsun': ['Ġ', 's', 'u', 'n'], 'Ġset': ['Ġ', 's', 'e', 't'], 'Ġchildren': ['Ġ', 'c', 'h', 'i', 'l', 'd', 'r', 'e', 'n'], 'Ġplayed': ['Ġ', 'p', 'l', 'a', 'y', 'e', 'd'], 'Ġgames': ['Ġ', 'g', 'a', 'm', 'e', 's'], 'Ġlaughed': ['Ġ', 'l', 'a', 'u', 'g', 'he', 'd'], 'Ġtogether': ['Ġ', 't', 'o', 'g', 'e', 't', 'he', 'r'], 'Ġgrassy': ['Ġ', 'g', 'r', 'a', 's', 's', 'y'], 'Ġfield': ['Ġ', 'f', 'i', 'e', 'l', 'd']}\n"
     ]
    }
   ],
   "source": [
    "splits = merge_pairs(\"h\",\"e\",splits)\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=50\n",
    "\n",
    "while len(vocab)<vocab_size:\n",
    "    pair_freqs = compute_pair_freq(splits)\n",
    "    best_pair=\"\"\n",
    "    max_freq=-1\n",
    "    for pair,freq in pair_freqs.items():\n",
    "        if freq>max_freq:\n",
    "            max_freq=freq\n",
    "            best_pair=pair\n",
    "    splits = merge_pairs(*best_pair,splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0]+best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('h', 'e'): 'he', ('Ġ', 't'): 'Ġt', ('Ġt', 'he'): 'Ġthe', ('Ġ', 's'): 'Ġs', ('Ġ', 'o'): 'Ġo', ('r', 'e'): 're', ('Ġ', 'a'): 'Ġa', ('Ġ', 'b'): 'Ġb', ('Ġ', 'w'): 'Ġw', ('i', 'n'): 'in', ('Ġ', 'f'): 'Ġf', ('a', 't'): 'at', ('i', 'e'): 'ie', ('c', 'h'): 'ch', ('o', 'o'): 'oo', ('Ġ', 'p'): 'Ġp', ('a', 'r'): 'ar', ('e', 'd'): 'ed'}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'A', 'H', 'S', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'w', 'y', 'z', 'Ġ', 'he', 'Ġt', 'Ġthe', 'Ġs', 'Ġo', 're', 'Ġa', 'Ġb', 'Ġw', 'in', 'Ġf', 'at', 'ie', 'ch', 'oo', 'Ġp', 'ar', 'ed']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word,offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair,merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i<len(split)-1:\n",
    "                if split[i]==pair[0] and split[i+1]==pair[1]:\n",
    "                    split = split[:i]+[merge]+split[i+2:]\n",
    "                else:\n",
    "                    i+=1\n",
    "            splits[idx]=split\n",
    "\n",
    "    return sum(splits,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'he', 'Ġ', 'c', 'at', 'Ġs', 'at', 'Ġb', 'y', 'Ġthe', 'Ġw', 'in', 'd', 'o', 'w', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"The cat sat by the window.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
